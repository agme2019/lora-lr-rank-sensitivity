{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47307fc6",
   "metadata": {},
   "source": [
    "# Learning Rate Matters: A Hands-On Comparison of LoRA Variants\n",
    "\n",
    "**Paper:** [Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning](https://arxiv.org/abs/2402.04998)\n",
    "**Authors:** Yu-Ang Lee, Ching-Yun Ko, Pin-Yu Chen, Mi-Yen Yeh\n",
    "\n",
    "### Paper Overview\n",
    "\n",
    "This paper investigates the burgeoning field of Low-Rank Adaptation (LoRA) variants for parameter-efficient fine-tuning (PEFT) of large language models. Many new methods (like PiSSA, DoRA, etc.) have been proposed, claiming substantial performance improvements over the original \"vanilla\" LoRA. The authors hypothesize that these reported gains might be an artifact of using fixed or poorly tuned hyperparameters, particularly the learning rate. Through an extensive hyperparameter search across multiple models and tasks, they demonstrate a crucial finding: **once the learning rate is properly tuned for each method individually, the performance gap between vanilla LoRA and its more complex variants largely disappears.** The paper concludes that vanilla LoRA remains a powerful and competitive baseline, and that new methods should be benchmarked under a fair, method-specific hyperparameter search to validate their claims.\n",
    "\n",
    "### What We'll Implement\n",
    "\n",
    "In this notebook, we will faithfully replicate the core message of the paper at a reduced, educational scale. We will:\n",
    "1.  **Build a small, decoder-only Transformer model** from scratch using PyTorch.\n",
    "2.  **Implement five LoRA-based PEFT methods** described in the paper:\n",
    "    *   **Vanilla LoRA**: The original baseline.\n",
    "    *   **PiSSA**: An initialization variant using top principal SVD components.\n",
    "    *   **MiLoRA**: An initialization variant using bottom minor SVD components.\n",
    "    *   **Init[AB]**: An initialization variant where both LoRA matrices are randomized.\n",
    "    *   **DoRA**: An architectural variant that decouples magnitude and direction.\n",
    "3.  **Create a synthetic mathematical reasoning dataset** to fine-tune our model.\n",
    "4.  **Run a systematic experiment** by sweeping the learning rate for each of the five methods.\n",
    "5.  **Visualize the results** to see if we can reproduce the paper's main conclusion: that all methods achieve similar peak performance, just at different optimal learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24671bb",
   "metadata": {},
   "source": [
    "### Problem Intuition\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) aims to adapt large, pretrained models to new tasks without the prohibitive cost of retraining all billions of parameters. **LoRA** is the most popular PEFT technique. It works by freezing the original model and injecting small, \"low-rank\" matrices (`A` and `B`) into the layers. Only these new, tiny matrices are trained, saving immense amounts of memory and computation.\n",
    "\n",
    "Recently, an \"arms race\" of new LoRA variants has emerged, each proposing a clever new initialization or architectural tweak and reporting better performance than the original. This paper questions the validity of these comparisons.\n",
    "\n",
    "**The Key Insight: An Analogy**\n",
    "\n",
    "Imagine you are comparing two race cars, a standard model (Vanilla LoRA) and a new, modified one (a LoRA variant). To find out which is faster, you hold a race. \n",
    "\n",
    "*   **Unfair Race:** You hire a professional driver for the new car but put an amateur behind the wheel of the standard one. The new car wins easily. Can you conclude the car is better? No, the driver might be the reason for the win.\n",
    "\n",
    "*   **Fair Race:** You hire two professional drivers, one for each car, and let them practice to find the best way to handle their specific vehicle. Now, when they race, you are truly comparing the cars themselves.\n",
    "\n",
    "In this analogy, the **car is the LoRA method**, and the **driver is the learning rate**. The paper argues that most prior work conducted an \"unfair race\" by using a single, fixed learning rate for all methods. Since different methods have different optimization landscapes (some are \"sharper\" or \"flatter\"), they require different learning rates to perform optimally. This paper runs the \"fair race\": it finds the best learning rate for each method before comparing their peak performance. The surprising result is that in a fair race, all the cars perform almost identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe43af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n",
      "torch: 2.10.0\n",
      "python: 3.11.7 (v3.11.7:fa7a6f2303, Dec  4 2023, 15:22:56) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "executable: /Users/agnivogosai/Desktop/LORA-TEST/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import os, math, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"python:\", __import__(\"sys\").version)\n",
    "print(\"executable:\", __import__(\"sys\").executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83fbd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveAtK(Dataset):\n",
    "    def __init__(self, n, seq_len, vocab, k, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.x = rng.integers(0, vocab, size=(n, seq_len), dtype=np.int64)\n",
    "        self.y = self.x[:, k].copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.x[idx], dtype=torch.long),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def make_loaders(\n",
    "    seq_len=32, vocab=64,\n",
    "    k_pre=5, k_ft=23,\n",
    "    n_pre_train=6000, n_pre_val=1500,\n",
    "    n_ft_train=2000, n_ft_val=800,\n",
    "    batch_size=64, seed=0\n",
    "):\n",
    "    pre_train = RetrieveAtK(n_pre_train, seq_len, vocab, k_pre, seed)\n",
    "    pre_val   = RetrieveAtK(n_pre_val,   seq_len, vocab, k_pre, seed+1)\n",
    "    ft_train  = RetrieveAtK(n_ft_train,  seq_len, vocab, k_ft,  seed+2)\n",
    "    ft_val    = RetrieveAtK(n_ft_val,    seq_len, vocab, k_ft,  seed+3)\n",
    "\n",
    "    return (\n",
    "        DataLoader(pre_train, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(pre_val,   batch_size=batch_size),\n",
    "        DataLoader(ft_train,  batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(ft_val,    batch_size=batch_size),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8845d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale / (x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt()\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        att = att.softmax(dim=-1)\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(d_model, n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.n1 = RMSNorm(d_model)\n",
    "        self.n2 = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.n1(x))\n",
    "        x = x + self.mlp(self.n2(x))\n",
    "        return x\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab, seq_len, d_model=128, n_layers=3, n_heads=4, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.emb(x) + self.pos[:, :x.size(1)]\n",
    "        for b in self.blocks:\n",
    "            h = b(h)\n",
    "        h = self.norm(h)\n",
    "        return self.head(h[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d838dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIXED LoRA injection (two-pass, no recursion) ---\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.A = nn.Linear(linear.in_features, r, bias=False)\n",
    "        self.B = nn.Linear(r, linear.out_features, bias=False)\n",
    "\n",
    "        # paper-style vanilla init: A ~ N(0, 0.02), B = 0\n",
    "        nn.init.normal_(self.A.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "\n",
    "    @property\n",
    "    def weight(self):  # keep Linear-like interface\n",
    "        return self.linear.weight\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.linear.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.B(self.A(x)) * self.scaling\n",
    "\n",
    "\n",
    "def inject_lora(model: nn.Module, r: int, skip_names=(\"head\",)):\n",
    "    \"\"\"\n",
    "    Safe injection: first collect (parent, child_name, child) then replace.\n",
    "    Avoids recursion errors from mutating module tree while iterating.\n",
    "\n",
    "    skip_names: tuple of child module attribute names to skip (e.g., keep classifier head plain Linear)\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "\n",
    "    # Pass 1: collect targets\n",
    "    for parent in model.modules():\n",
    "        for name, child in parent.named_children():\n",
    "            if name in skip_names:\n",
    "                continue\n",
    "            if isinstance(child, nn.Linear) and not isinstance(child, LoRALinear):\n",
    "                targets.append((parent, name, child))\n",
    "\n",
    "    # Pass 2: replace\n",
    "    for parent, name, child in targets:\n",
    "        setattr(parent, name, LoRALinear(child, r=r, alpha=r))  # alpha=r (paper setting)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3943963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DoRA + PiSSA-like + MiLoRA-like adapters (clean + device-safe) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified DoRA:\n",
    "    - learn low-rank direction update B@A\n",
    "    - decouple magnitude via per-output m\n",
    "    \"\"\"\n",
    "    def __init__(self, linear: nn.Linear, r: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.A = nn.Linear(linear.in_features, r, bias=False)\n",
    "        self.B = nn.Linear(r, linear.out_features, bias=False)\n",
    "        self.m = nn.Parameter(torch.ones(linear.out_features))\n",
    "\n",
    "        # vanilla init (same spirit as LoRA)\n",
    "        nn.init.normal_(self.A.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "\n",
    "    @property\n",
    "    def weight(self):  # keep Linear-like interface\n",
    "        return self.linear.weight\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.linear.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = self.linear.weight                       # [out, in]\n",
    "        delta = (self.B.weight @ self.A.weight) * self.scaling\n",
    "        W_eff = W + delta\n",
    "        W_dir = F.normalize(W_eff, dim=1)            # row-wise normalize\n",
    "        y = x @ (W_dir * self.m.unsqueeze(1)).t()    # apply magnitude\n",
    "        if self.linear.bias is not None:\n",
    "            y = y + self.linear.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "def _svd_init_lora(wrapper, mode: str = \"top\"):\n",
    "    \"\"\"\n",
    "    Initialize LoRA factors so that (B@A) approximates:\n",
    "    - top-r singular components (PiSSA-like)\n",
    "    - bottom-r singular components (MiLoRA-like)\n",
    "\n",
    "    wrapper must be a LoRALinear-like object with:\n",
    "      wrapper.linear.weight, wrapper.A.weight, wrapper.B.weight, wrapper.r\n",
    "    \"\"\"\n",
    "    W = wrapper.linear.weight.detach().float().cpu()  # [out, in]\n",
    "\n",
    "    # robust SVD across torch versions\n",
    "    try:\n",
    "        U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "    except Exception:\n",
    "        # older torch fallback\n",
    "        U, S, V = torch.svd(W)\n",
    "        Vh = V.t()\n",
    "\n",
    "    r = wrapper.r\n",
    "    if r > min(W.shape):\n",
    "        raise ValueError(f\"rank r={r} too large for weight shape {tuple(W.shape)}\")\n",
    "\n",
    "    if mode == \"top\":\n",
    "        idx = torch.arange(r)\n",
    "    elif mode == \"bottom\":\n",
    "        idx = torch.arange(len(S) - r, len(S))\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'top' or 'bottom'\")\n",
    "\n",
    "    U_r = U[:, idx]          # [out, r]\n",
    "    S_r = S[idx]             # [r]\n",
    "    V_r = Vh[idx, :]         # [r, in]\n",
    "\n",
    "    # set A,B so B@A ≈ U_r diag(S_r) V_r\n",
    "    # choose A = sqrt(S) V, B = U sqrt(S)\n",
    "    sqrtS = torch.sqrt(S_r)\n",
    "    A = (sqrtS.unsqueeze(1) * V_r)                  # [r, in]\n",
    "    B = (U_r * sqrtS.unsqueeze(0))                  # [out, r]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        wrapper.A.weight.copy_(A.to(wrapper.A.weight.device))\n",
    "        wrapper.B.weight.copy_(B.to(wrapper.B.weight.device))\n",
    "\n",
    "\n",
    "def inject_adapter(model: nn.Module, kind: str, r: int, skip_names=(\"head\",)):\n",
    "    \"\"\"\n",
    "    kind: 'lora' | 'dora' | 'pissa' | 'milora'\n",
    "    Uses TWO-PASS replacement to avoid recursion issues.\n",
    "    \"\"\"\n",
    "    assert kind in (\"lora\", \"dora\", \"pissa\", \"milora\")\n",
    "    targets = []\n",
    "\n",
    "    # pass 1: collect target linears\n",
    "    for parent in model.modules():\n",
    "        for name, child in parent.named_children():\n",
    "            if name in skip_names:\n",
    "                continue\n",
    "            if isinstance(child, nn.Linear):\n",
    "                # avoid double-wrapping\n",
    "                if isinstance(child, (LoRALinear, DoRALinear)):\n",
    "                    continue\n",
    "                targets.append((parent, name, child))\n",
    "\n",
    "    # pass 2: replace\n",
    "    for parent, name, child in targets:\n",
    "        if kind == \"lora\":\n",
    "            wrapped = LoRALinear(child, r=r, alpha=r)\n",
    "        elif kind == \"dora\":\n",
    "            wrapped = DoRALinear(child, r=r, alpha=r)\n",
    "        elif kind == \"pissa\":\n",
    "            wrapped = LoRALinear(child, r=r, alpha=r)\n",
    "            _svd_init_lora(wrapped, mode=\"top\")\n",
    "        elif kind == \"milora\":\n",
    "            wrapped = LoRALinear(child, r=r, alpha=r)\n",
    "            _svd_init_lora(wrapped, mode=\"bottom\")\n",
    "\n",
    "        setattr(parent, name, wrapped)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6ab83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining base...\n",
      "Pretrain val acc: 1.0\n",
      "6.32e-03 -> 0.0167\n",
      "3.56e-03 -> 0.0292\n",
      "2.00e-03 -> 0.0292\n",
      "1.12e-03 -> 0.0400\n",
      "6.32e-04 -> 0.0575\n",
      "3.56e-04 -> 0.0617\n",
      "2.00e-04 -> 0.0675\n",
      "1.12e-04 -> 0.0717\n",
      "6.32e-05 -> 0.0225\n",
      "3.56e-05 -> 0.0175\n",
      "2.00e-05 -> 0.0192\n",
      "1.12e-05 -> 0.0175\n",
      "6.32e-06 -> 0.0158\n",
      "3.56e-06 -> 0.0158\n",
      "2.00e-06 -> 0.0158\n",
      "1.12e-06 -> 0.0158\n",
      "Done. len(LR_GRID)= 16 len(accs)= 16\n"
     ]
    }
   ],
   "source": [
    "# --- CLEAN Training + LR sweep (creates LR_GRID and accs) ---\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(x).argmax(dim=-1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def train_epochs(model, loader, lr, epochs, weight_decay=0.0):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "# 1) Data\n",
    "set_seed(0)\n",
    "pre_tr, pre_va, ft_tr, ft_va = make_loaders(\n",
    "    seq_len=32, vocab=64,\n",
    "    k_pre=5, k_ft=23,\n",
    "    n_pre_train=12000, n_pre_val=3000,\n",
    "    n_ft_train=4000,  n_ft_val=1200,\n",
    "    batch_size=64, seed=0\n",
    ")\n",
    "\n",
    "# 2) Pretrain base (full model trainable)\n",
    "base = TinyTransformer(vocab=64, seq_len=32).to(DEVICE)\n",
    "print(\"Pretraining base...\")\n",
    "train_epochs(base, pre_tr, lr=3e-4, epochs=10, weight_decay=0.01)\n",
    "print(\"Pretrain val acc:\", eval_acc(base, pre_va))\n",
    "\n",
    "state = {k: v.detach().cpu().clone() for k, v in base.state_dict().items()}\n",
    "\n",
    "# 3) Paper-style LR grid (16 points)\n",
    "multipliers = [1.1247, 2.0, 3.5566, 6.3246]\n",
    "decades = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "LR_GRID = sorted([m*d for d in decades for m in multipliers], reverse=True)\n",
    "\n",
    "# 4) LR sweep: adapter-only finetune\n",
    "accs = []\n",
    "for lr in LR_GRID:\n",
    "    model = TinyTransformer(vocab=64, seq_len=32)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "\n",
    "    # inject LoRA (creates new params on CPU)\n",
    "    inject_lora(model, r=16, skip_names=(\"head\",))\n",
    "\n",
    "    # move everything (including LoRA params) to DEVICE\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # freeze everything\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # unfreeze LoRA params\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, LoRALinear):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # CRITICAL: unfreeze head\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # finetune\n",
    "    train_epochs(model, ft_tr, lr=lr, epochs=10, weight_decay=0.0)\n",
    "    acc = eval_acc(model, ft_va)\n",
    "    accs.append(acc)\n",
    "    print(f\"{lr:.2e} -> {acc:.4f}\")\n",
    "\n",
    "print(\"Done. len(LR_GRID)=\", len(LR_GRID), \"len(accs)=\", len(accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ff8c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== LoRA (vanilla) ====\n",
      "6.32e-03 -> 0.0208\n",
      "3.56e-03 -> 0.0175\n",
      "2.00e-03 -> 0.0267\n",
      "1.12e-03 -> 0.0417\n",
      "6.32e-04 -> 0.0517\n",
      "3.56e-04 -> 0.0592\n",
      "2.00e-04 -> 0.0667\n",
      "1.12e-04 -> 0.0633\n",
      "6.32e-05 -> 0.0383\n",
      "3.56e-05 -> 0.0167\n",
      "2.00e-05 -> 0.0175\n",
      "1.12e-05 -> 0.0150\n",
      "6.32e-06 -> 0.0158\n",
      "3.56e-06 -> 0.0158\n",
      "2.00e-06 -> 0.0158\n",
      "1.12e-06 -> 0.0158\n",
      "\n",
      "==== DoRA ====\n",
      "6.32e-03 -> 0.0242\n",
      "3.56e-03 -> 0.0267\n",
      "2.00e-03 -> 0.0242\n",
      "1.12e-03 -> 0.0333\n",
      "6.32e-04 -> 0.0333\n",
      "3.56e-04 -> 0.0317\n",
      "2.00e-04 -> 0.0258\n",
      "1.12e-04 -> 0.0200\n",
      "6.32e-05 -> 0.0175\n",
      "3.56e-05 -> 0.0175\n",
      "2.00e-05 -> 0.0175\n",
      "1.12e-05 -> 0.0200\n",
      "6.32e-06 -> 0.0192\n",
      "3.56e-06 -> 0.0175\n",
      "2.00e-06 -> 0.0158\n",
      "1.12e-06 -> 0.0158\n",
      "\n",
      "==== PiSSA-like (top-r SVD init) ====\n",
      "6.32e-03 -> 0.0167\n",
      "3.56e-03 -> 0.0225\n",
      "2.00e-03 -> 0.0325\n",
      "1.12e-03 -> 0.0267\n",
      "6.32e-04 -> 0.0225\n",
      "3.56e-04 -> 0.0233\n",
      "2.00e-04 -> 0.0333\n",
      "1.12e-04 -> 0.0250\n",
      "6.32e-05 -> 0.0200\n",
      "3.56e-05 -> 0.0175\n",
      "2.00e-05 -> 0.0167\n",
      "1.12e-05 -> 0.0150\n",
      "6.32e-06 -> 0.0150\n",
      "3.56e-06 -> 0.0167\n",
      "2.00e-06 -> 0.0167\n",
      "1.12e-06 -> 0.0167\n",
      "\n",
      "==== MiLoRA-like (bottom-r SVD init) ====\n",
      "6.32e-03 -> 0.0133\n",
      "3.56e-03 -> 0.0300\n",
      "2.00e-03 -> 0.0208\n",
      "1.12e-03 -> 0.0458\n",
      "6.32e-04 -> 0.0425\n",
      "3.56e-04 -> 0.0625\n",
      "2.00e-04 -> 0.0525\n",
      "1.12e-04 -> 0.0342\n",
      "6.32e-05 -> 0.0192\n",
      "3.56e-05 -> 0.0175\n",
      "2.00e-05 -> 0.0175\n",
      "1.12e-05 -> 0.0183\n",
      "6.32e-06 -> 0.0158\n",
      "3.56e-06 -> 0.0158\n",
      "2.00e-06 -> 0.0158\n",
      "1.12e-06 -> 0.0158\n",
      "\n",
      "Done. You now have `results` dict for multi-method plotting.\n"
     ]
    }
   ],
   "source": [
    "# --- Multi-method LR sweep: LoRA vs DoRA vs PiSSA vs MiLoRA ---\n",
    "\n",
    "methods = [\n",
    "    (\"lora\",   \"LoRA (vanilla)\"),\n",
    "    (\"dora\",   \"DoRA\"),\n",
    "    (\"pissa\",  \"PiSSA-like (top-r SVD init)\"),\n",
    "    (\"milora\", \"MiLoRA-like (bottom-r SVD init)\"),\n",
    "]\n",
    "\n",
    "rank = 16  # you can try 8 too for stronger LR sensitivity\n",
    "\n",
    "results = {}\n",
    "\n",
    "for kind, name in methods:\n",
    "    print(\"\\n====\", name, \"====\")\n",
    "    results[kind] = {\"name\": name, \"lrs\": [], \"best_val_acc\": []}\n",
    "\n",
    "    for lr in LR_GRID:\n",
    "        # rebuild from pretrained base\n",
    "        model = TinyTransformer(vocab=64, seq_len=32)\n",
    "        model.load_state_dict(state, strict=True)\n",
    "\n",
    "        # inject adapter (creates params on CPU)\n",
    "        inject_adapter(model, kind=kind, r=rank, skip_names=(\"head\",))\n",
    "\n",
    "        # move to DEVICE AFTER injection (critical on MPS)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        # freeze everything\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # unfreeze adapter params + head\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (LoRALinear, DoRALinear)):\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        for p in model.head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # finetune (same as before)\n",
    "        train_epochs(model, ft_tr, lr=lr, epochs=10, weight_decay=0.0)\n",
    "        acc = eval_acc(model, ft_va)\n",
    "\n",
    "        results[kind][\"lrs\"].append(lr)\n",
    "        results[kind][\"best_val_acc\"].append(float(acc))\n",
    "\n",
    "        print(f\"{lr:.2e} -> {acc:.4f}\")\n",
    "\n",
    "print(\"\\nDone. You now have `results` dict for multi-method plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97947f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/c_hmq0zs65x8xr0xmqp67lcm0000gn/T/ipykernel_61551/1653307472.py:20: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/2f/c_hmq0zs65x8xr0xmqp67lcm0000gn/T/ipykernel_61551/1653307472.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# --- Plot multi-method results ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.5))\n",
    "for k, v in results.items():\n",
    "    lrs = np.array(v[\"lrs\"], dtype=float)\n",
    "    acc = np.array(v[\"best_val_acc\"], dtype=float)\n",
    "    order = np.argsort(lrs)\n",
    "    plt.plot(lrs[order], acc[order], marker=\"o\", label=v[\"name\"])\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate (log scale)\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"LR sweep parity (synthetic)\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# best-of-LR bar plot\n",
    "names = [v[\"name\"] for v in results.values()]\n",
    "bests = [float(np.max(v[\"best_val_acc\"])) for v in results.values()]\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.5))\n",
    "plt.bar(np.arange(len(names)), bests)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Best val accuracy (over LR sweep)\")\n",
    "plt.title(\"Best-of-LR comparison\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5642e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots:\n",
      " - artifacts/lr_sweep_parity.png\n",
      " - artifacts/best_of_lr.png\n"
     ]
    }
   ],
   "source": [
    "# --- Save LR-sweep plots to disk (no interactive display) ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_dir = \"artifacts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 1) Multi-method LR curves\n",
    "plt.figure(figsize=(7.5, 4.5))\n",
    "for k, v in results.items():\n",
    "    lrs = np.array(v[\"lrs\"], dtype=float)\n",
    "    acc = np.array(v[\"best_val_acc\"], dtype=float)\n",
    "    order = np.argsort(lrs)\n",
    "    plt.plot(lrs[order], acc[order], marker=\"o\", label=v[\"name\"])\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate (log scale)\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"LR sweep parity (synthetic)\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "path_lr = os.path.join(out_dir, \"lr_sweep_parity.png\")\n",
    "plt.savefig(path_lr, dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# 2) Best-of-LR bar plot\n",
    "names = [v[\"name\"] for v in results.values()]\n",
    "bests = [float(np.max(v[\"best_val_acc\"])) for v in results.values()]\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.5))\n",
    "plt.bar(np.arange(len(names)), bests)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Best validation accuracy (over LR sweep)\")\n",
    "plt.title(\"Best-of-LR comparison\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "path_best = os.path.join(out_dir, \"best_of_lr.png\")\n",
    "plt.savefig(path_best, dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved plots:\")\n",
    "print(\" -\", path_lr)\n",
    "print(\" -\", path_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca16c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has LR_GRID? True\n",
      "Has accs? True\n",
      "len(LR_GRID) = 16\n",
      "len(accs) = 16\n",
      "accs preview: [0.016666666666666666, 0.029166666666666667, 0.029166666666666667, 0.04, 0.0575]\n",
      "Saved: artifacts/single_acc_vs_lr.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/c_hmq0zs65x8xr0xmqp67lcm0000gn/T/ipykernel_61551/2300986984.py:59: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts/single_acc_vs_lr.png'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Diagnose + plot LR sweep (robust) ---\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "def _is_numeric_list(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)) and len(x) > 0:\n",
    "        try:\n",
    "            a = np.array(x, dtype=float)\n",
    "            return np.isfinite(a).any()\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def _find_candidate_acc_list(LR_GRID):\n",
    "    \"\"\"Try to find a list/array of numeric values with same length as LR_GRID in globals().\"\"\"\n",
    "    n = len(LR_GRID)\n",
    "    candidates = []\n",
    "    for k, v in globals().items():\n",
    "        if k.startswith(\"_\"):\n",
    "            continue\n",
    "        if isinstance(v, (list, tuple, np.ndarray)) and len(v) == n:\n",
    "            try:\n",
    "                arr = np.array(v, dtype=float)\n",
    "                if np.isfinite(arr).any():\n",
    "                    candidates.append((k, arr))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return candidates\n",
    "\n",
    "def plot_single(LR_GRID, accs, title=\"LR sweep\", out_prefix=\"single\"):\n",
    "    lrs = np.array(LR_GRID, dtype=float)\n",
    "    acc = np.array(accs, dtype=float)\n",
    "\n",
    "    # Guard: handle length mismatch safely\n",
    "    if len(acc) == 0:\n",
    "        raise RuntimeError(\"`accs` is empty. Your sweep didn't populate it (or it crashed before appending).\")\n",
    "\n",
    "    n = min(len(lrs), len(acc))\n",
    "    lrs = lrs[:n]\n",
    "    acc = acc[:n]\n",
    "\n",
    "    order = np.argsort(lrs)\n",
    "\n",
    "    plt.figure(figsize=(7.5, 4.5))\n",
    "    plt.plot(lrs[order], acc[order], marker=\"o\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Learning rate (log scale)\")\n",
    "    plt.ylabel(\"Validation accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = f\"artifacts/{out_prefix}_acc_vs_lr.png\"\n",
    "    plt.savefig(out, dpi=180)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out)\n",
    "    return out\n",
    "\n",
    "# ---- Print diagnostics ----\n",
    "print(\"Has LR_GRID?\", \"LR_GRID\" in globals())\n",
    "print(\"Has accs?\", \"accs\" in globals())\n",
    "if \"LR_GRID\" in globals():\n",
    "    print(\"len(LR_GRID) =\", len(LR_GRID))\n",
    "if \"accs\" in globals():\n",
    "    try:\n",
    "        print(\"len(accs) =\", len(accs))\n",
    "        print(\"accs preview:\", accs[:5])\n",
    "    except Exception as e:\n",
    "        print(\"Could not preview accs:\", e)\n",
    "\n",
    "# ---- Auto-fix if accs is empty by finding another variable ----\n",
    "if \"LR_GRID\" not in globals():\n",
    "    raise RuntimeError(\"LR_GRID not found. Run your LR sweep cell first.\")\n",
    "\n",
    "if \"accs\" not in globals() or (isinstance(accs, (list, tuple, np.ndarray)) and len(accs) == 0):\n",
    "    print(\"\\n`accs` is missing/empty. Searching for another accuracy list with same length as LR_GRID...\")\n",
    "    candidates = _find_candidate_acc_list(LR_GRID)\n",
    "    if len(candidates) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"I couldn't find any numeric list in memory with the same length as LR_GRID.\\n\"\n",
    "            \"Fix your sweep cell to append into `accs`, e.g. `accs.append(acc)` inside the LR loop,\\n\"\n",
    "            \"then rerun the sweep and come back to this plot cell.\"\n",
    "        )\n",
    "    print(\"Found candidates:\", [k for k,_ in candidates])\n",
    "    # pick the first candidate\n",
    "    name, arr = candidates[0]\n",
    "    print(f\"Using `{name}` as accs for plotting.\")\n",
    "    accs = arr.tolist()\n",
    "\n",
    "# ---- Finally plot ----\n",
    "plot_single(LR_GRID, accs, title=\"LR sweep\", out_prefix=\"single\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ea9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "METHOD: LoRA (vanilla)\n",
      "\n",
      "-- rank r=2 --\n",
      "  lr=6.32e-03  acc=0.0192\n",
      "  lr=3.56e-03  acc=0.0283\n",
      "  lr=2.00e-03  acc=0.0342\n",
      "  lr=1.12e-03  acc=0.0392\n",
      "  lr=6.32e-04  acc=0.0458\n",
      "  lr=3.56e-04  acc=0.0617\n",
      "  lr=2.00e-04  acc=0.0600\n",
      "  lr=1.12e-04  acc=0.0158\n",
      "  lr=6.32e-05  acc=0.0158\n",
      "  lr=3.56e-05  acc=0.0167\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0183\n",
      "  lr=6.32e-06  acc=0.0158\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=2: acc=0.0617 at lr=3.56e-04\n",
      "\n",
      "-- rank r=4 --\n",
      "  lr=6.32e-03  acc=0.0167\n",
      "  lr=3.56e-03  acc=0.0292\n",
      "  lr=2.00e-03  acc=0.0267\n",
      "  lr=1.12e-03  acc=0.0383\n",
      "  lr=6.32e-04  acc=0.0558\n",
      "  lr=3.56e-04  acc=0.0575\n",
      "  lr=2.00e-04  acc=0.0558\n",
      "  lr=1.12e-04  acc=0.0225\n",
      "  lr=6.32e-05  acc=0.0158\n",
      "  lr=3.56e-05  acc=0.0175\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0167\n",
      "  lr=6.32e-06  acc=0.0158\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=4: acc=0.0575 at lr=3.56e-04\n",
      "\n",
      "-- rank r=8 --\n",
      "  lr=6.32e-03  acc=0.0317\n",
      "  lr=3.56e-03  acc=0.0242\n",
      "  lr=2.00e-03  acc=0.0258\n",
      "  lr=1.12e-03  acc=0.0483\n",
      "  lr=6.32e-04  acc=0.0492\n",
      "  lr=3.56e-04  acc=0.0642\n",
      "  lr=2.00e-04  acc=0.0567\n",
      "  lr=1.12e-04  acc=0.0567\n",
      "  lr=6.32e-05  acc=0.0175\n",
      "  lr=3.56e-05  acc=0.0167\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0167\n",
      "  lr=6.32e-06  acc=0.0158\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=8: acc=0.0642 at lr=3.56e-04\n",
      "\n",
      "-- rank r=16 --\n",
      "  lr=6.32e-03  acc=0.0158\n",
      "  lr=3.56e-03  acc=0.0217\n",
      "  lr=2.00e-03  acc=0.0325\n",
      "  lr=1.12e-03  acc=0.0342\n",
      "  lr=6.32e-04  acc=0.0600\n",
      "  lr=3.56e-04  acc=0.0600\n",
      "  lr=2.00e-04  acc=0.0617\n",
      "  lr=1.12e-04  acc=0.0575\n",
      "  lr=6.32e-05  acc=0.0200\n",
      "  lr=3.56e-05  acc=0.0200\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0167\n",
      "  lr=6.32e-06  acc=0.0158\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=16: acc=0.0617 at lr=2.00e-04\n",
      "\n",
      "-- rank r=32 --\n",
      "  lr=6.32e-03  acc=0.0258\n",
      "  lr=3.56e-03  acc=0.0275\n",
      "  lr=2.00e-03  acc=0.0267\n",
      "  lr=1.12e-03  acc=0.0458\n",
      "  lr=6.32e-04  acc=0.0500\n",
      "  lr=3.56e-04  acc=0.0650\n",
      "  lr=2.00e-04  acc=0.0658\n",
      "  lr=1.12e-04  acc=0.0708\n",
      "  lr=6.32e-05  acc=0.0417\n",
      "  lr=3.56e-05  acc=0.0183\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0183\n",
      "  lr=6.32e-06  acc=0.0167\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=32: acc=0.0708 at lr=1.12e-04\n",
      "\n",
      "==============================\n",
      "METHOD: DoRA\n",
      "\n",
      "-- rank r=2 --\n",
      "  lr=6.32e-03  acc=0.0192\n",
      "  lr=3.56e-03  acc=0.0250\n",
      "  lr=2.00e-03  acc=0.0333\n",
      "  lr=1.12e-03  acc=0.0258\n",
      "  lr=6.32e-04  acc=0.0217\n",
      "  lr=3.56e-04  acc=0.0183\n",
      "  lr=2.00e-04  acc=0.0250\n",
      "  lr=1.12e-04  acc=0.0217\n",
      "  lr=6.32e-05  acc=0.0183\n",
      "  lr=3.56e-05  acc=0.0158\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0192\n",
      "  lr=6.32e-06  acc=0.0192\n",
      "  lr=3.56e-06  acc=0.0167\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=2: acc=0.0333 at lr=2.00e-03\n",
      "\n",
      "-- rank r=4 --\n",
      "  lr=6.32e-03  acc=0.0267\n",
      "  lr=3.56e-03  acc=0.0342\n",
      "  lr=2.00e-03  acc=0.0275\n",
      "  lr=1.12e-03  acc=0.0358\n",
      "  lr=6.32e-04  acc=0.0217\n",
      "  lr=3.56e-04  acc=0.0358\n",
      "  lr=2.00e-04  acc=0.0308\n",
      "  lr=1.12e-04  acc=0.0133\n",
      "  lr=6.32e-05  acc=0.0175\n",
      "  lr=3.56e-05  acc=0.0158\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0192\n",
      "  lr=6.32e-06  acc=0.0192\n",
      "  lr=3.56e-06  acc=0.0167\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=4: acc=0.0358 at lr=1.12e-03\n",
      "\n",
      "-- rank r=8 --\n",
      "  lr=6.32e-03  acc=0.0192\n",
      "  lr=3.56e-03  acc=0.0300\n",
      "  lr=2.00e-03  acc=0.0308\n",
      "  lr=1.12e-03  acc=0.0308\n",
      "  lr=6.32e-04  acc=0.0250\n",
      "  lr=3.56e-04  acc=0.0233\n",
      "  lr=2.00e-04  acc=0.0258\n",
      "  lr=1.12e-04  acc=0.0200\n",
      "  lr=6.32e-05  acc=0.0183\n",
      "  lr=3.56e-05  acc=0.0167\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0192\n",
      "  lr=6.32e-06  acc=0.0183\n",
      "  lr=3.56e-06  acc=0.0167\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=8: acc=0.0308 at lr=2.00e-03\n",
      "\n",
      "-- rank r=16 --\n",
      "  lr=6.32e-03  acc=0.0100\n",
      "  lr=3.56e-03  acc=0.0317\n",
      "  lr=2.00e-03  acc=0.0208\n",
      "  lr=1.12e-03  acc=0.0325\n",
      "  lr=6.32e-04  acc=0.0325\n",
      "  lr=3.56e-04  acc=0.0300\n",
      "  lr=2.00e-04  acc=0.0200\n",
      "  lr=1.12e-04  acc=0.0192\n",
      "  lr=6.32e-05  acc=0.0183\n",
      "  lr=3.56e-05  acc=0.0167\n",
      "  lr=2.00e-05  acc=0.0183\n",
      "  lr=1.12e-05  acc=0.0200\n",
      "  lr=6.32e-06  acc=0.0183\n",
      "  lr=3.56e-06  acc=0.0175\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=16: acc=0.0325 at lr=1.12e-03\n",
      "\n",
      "-- rank r=32 --\n",
      "  lr=6.32e-03  acc=0.0225\n",
      "  lr=3.56e-03  acc=0.0233\n",
      "  lr=2.00e-03  acc=0.0317\n",
      "  lr=1.12e-03  acc=0.0308\n",
      "  lr=6.32e-04  acc=0.0275\n",
      "  lr=3.56e-04  acc=0.0350\n",
      "  lr=2.00e-04  acc=0.0283\n",
      "  lr=1.12e-04  acc=0.0167\n",
      "  lr=6.32e-05  acc=0.0192\n",
      "  lr=3.56e-05  acc=0.0183\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0200\n",
      "  lr=6.32e-06  acc=0.0192\n",
      "  lr=3.56e-06  acc=0.0183\n",
      "  lr=2.00e-06  acc=0.0158\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=32: acc=0.0350 at lr=3.56e-04\n",
      "\n",
      "==============================\n",
      "METHOD: PiSSA-like (top-r SVD init)\n",
      "\n",
      "-- rank r=2 --\n",
      "  lr=6.32e-03  acc=0.0183\n",
      "  lr=3.56e-03  acc=0.0233\n",
      "  lr=2.00e-03  acc=0.0317\n",
      "  lr=1.12e-03  acc=0.0158\n",
      "  lr=6.32e-04  acc=0.0300\n",
      "  lr=3.56e-04  acc=0.0225\n",
      "  lr=2.00e-04  acc=0.0275\n",
      "  lr=1.12e-04  acc=0.0217\n",
      "  lr=6.32e-05  acc=0.0242\n",
      "  lr=3.56e-05  acc=0.0217\n",
      "  lr=2.00e-05  acc=0.0200\n",
      "  lr=1.12e-05  acc=0.0200\n",
      "  lr=6.32e-06  acc=0.0192\n",
      "  lr=3.56e-06  acc=0.0167\n",
      "  lr=2.00e-06  acc=0.0167\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=2: acc=0.0317 at lr=2.00e-03\n",
      "\n",
      "-- rank r=4 --\n",
      "  lr=6.32e-03  acc=0.0242\n",
      "  lr=3.56e-03  acc=0.0242\n",
      "  lr=2.00e-03  acc=0.0200\n",
      "  lr=1.12e-03  acc=0.0283\n",
      "  lr=6.32e-04  acc=0.0283\n",
      "  lr=3.56e-04  acc=0.0317\n",
      "  lr=2.00e-04  acc=0.0233\n",
      "  lr=1.12e-04  acc=0.0267\n",
      "  lr=6.32e-05  acc=0.0283\n",
      "  lr=3.56e-05  acc=0.0267\n",
      "  lr=2.00e-05  acc=0.0200\n",
      "  lr=1.12e-05  acc=0.0158\n",
      "  lr=6.32e-06  acc=0.0175\n",
      "  lr=3.56e-06  acc=0.0183\n",
      "  lr=2.00e-06  acc=0.0167\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=4: acc=0.0317 at lr=3.56e-04\n",
      "\n",
      "-- rank r=8 --\n",
      "  lr=6.32e-03  acc=0.0250\n",
      "  lr=3.56e-03  acc=0.0192\n",
      "  lr=2.00e-03  acc=0.0150\n",
      "  lr=1.12e-03  acc=0.0250\n",
      "  lr=6.32e-04  acc=0.0258\n",
      "  lr=3.56e-04  acc=0.0267\n",
      "  lr=2.00e-04  acc=0.0233\n",
      "  lr=1.12e-04  acc=0.0200\n",
      "  lr=6.32e-05  acc=0.0217\n",
      "  lr=3.56e-05  acc=0.0133\n",
      "  lr=2.00e-05  acc=0.0192\n",
      "  lr=1.12e-05  acc=0.0183\n",
      "  lr=6.32e-06  acc=0.0150\n",
      "  lr=3.56e-06  acc=0.0158\n",
      "  lr=2.00e-06  acc=0.0150\n",
      "  lr=1.12e-06  acc=0.0158\n",
      "BEST for r=8: acc=0.0267 at lr=3.56e-04\n",
      "\n",
      "-- rank r=16 --\n",
      "  lr=6.32e-03  acc=0.0175\n",
      "  lr=3.56e-03  acc=0.0192\n",
      "  lr=2.00e-03  acc=0.0225\n",
      "  lr=1.12e-03  acc=0.0242\n",
      "  lr=6.32e-04  acc=0.0208\n",
      "  lr=3.56e-04  acc=0.0258\n",
      "  lr=2.00e-04  acc=0.0208\n",
      "  lr=1.12e-04  acc=0.0192\n",
      "  lr=6.32e-05  acc=0.0175\n",
      "  lr=3.56e-05  acc=0.0158\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0158\n",
      "  lr=6.32e-06  acc=0.0150\n",
      "  lr=3.56e-06  acc=0.0167\n",
      "  lr=2.00e-06  acc=0.0167\n",
      "  lr=1.12e-06  acc=0.0167\n",
      "BEST for r=16: acc=0.0258 at lr=3.56e-04\n",
      "\n",
      "-- rank r=32 --\n",
      "  lr=6.32e-03  acc=0.0250\n",
      "  lr=3.56e-03  acc=0.0308\n",
      "  lr=2.00e-03  acc=0.0250\n",
      "  lr=1.12e-03  acc=0.0175\n",
      "  lr=6.32e-04  acc=0.0217\n",
      "  lr=3.56e-04  acc=0.0192\n",
      "  lr=2.00e-04  acc=0.0200\n",
      "  lr=1.12e-04  acc=0.0175\n",
      "  lr=6.32e-05  acc=0.0158\n",
      "  lr=3.56e-05  acc=0.0175\n",
      "  lr=2.00e-05  acc=0.0175\n",
      "  lr=1.12e-05  acc=0.0167\n",
      "  lr=6.32e-06  acc=0.0158\n",
      "  lr=3.56e-06  acc=0.0192\n",
      "  lr=2.00e-06  acc=0.0175\n",
      "  lr=1.12e-06  acc=0.0167\n",
      "BEST for r=32: acc=0.0308 at lr=3.56e-03\n",
      "\n",
      "==============================\n",
      "METHOD: MiLoRA-like (bottom-r SVD init)\n",
      "\n",
      "-- rank r=2 --\n",
      "  lr=6.32e-03  acc=0.0267\n",
      "  lr=3.56e-03  acc=0.0300\n",
      "  lr=2.00e-03  acc=0.0258\n",
      "  lr=1.12e-03  acc=0.0433\n"
     ]
    }
   ],
   "source": [
    "# --- Rank sweeps × LR sweeps ---\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# methods must match your inject_adapter(kind=...)\n",
    "methods = [\n",
    "    (\"lora\",   \"LoRA (vanilla)\"),\n",
    "    (\"dora\",   \"DoRA\"),\n",
    "    (\"pissa\",  \"PiSSA-like (top-r SVD init)\"),\n",
    "    (\"milora\", \"MiLoRA-like (bottom-r SVD init)\"),\n",
    "]\n",
    "\n",
    "# Pick ranks (adjust as needed)\n",
    "RANKS = [2, 4, 8, 16, 32]\n",
    "\n",
    "# Reuse LR_GRID from your notebook (paper-style 16 points)\n",
    "assert \"LR_GRID\" in globals() and len(LR_GRID) > 0, \"Run your LR grid cell first.\"\n",
    "assert \"state\" in globals(), \"Need pretrained `state` dict. Run pretraining cell first.\"\n",
    "assert \"ft_tr\" in globals() and \"ft_va\" in globals(), \"Need ft loaders. Run make_loaders cell first.\"\n",
    "\n",
    "# finetune epochs for each run (tradeoff time vs stability)\n",
    "FINETUNE_EPOCHS = 10\n",
    "\n",
    "results_rank = {}  # results_rank[kind][r] = {\"lrs\": [...], \"acc\": [...], \"best\": float, \"best_lr\": float}\n",
    "\n",
    "for kind, name in methods:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"METHOD:\", name)\n",
    "    results_rank[kind] = {\"name\": name, \"ranks\": {}}\n",
    "\n",
    "    for r in RANKS:\n",
    "        print(f\"\\n-- rank r={r} --\")\n",
    "        accs = []\n",
    "        for lr in LR_GRID:\n",
    "            # rebuild from pretrained base\n",
    "            model = TinyTransformer(vocab=64, seq_len=32)\n",
    "            model.load_state_dict(state, strict=True)\n",
    "\n",
    "            # inject adapter at this rank\n",
    "            inject_adapter(model, kind=kind, r=r, skip_names=(\"head\",))\n",
    "\n",
    "            # IMPORTANT: move AFTER injection (MPS safety)\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            # freeze everything\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # unfreeze adapter params + head\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, (LoRALinear, DoRALinear)):\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "            for p in model.head.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # finetune and eval\n",
    "            train_epochs(model, ft_tr, lr=lr, epochs=FINETUNE_EPOCHS, weight_decay=0.0)\n",
    "            acc = eval_acc(model, ft_va)\n",
    "            accs.append(float(acc))\n",
    "\n",
    "            print(f\"  lr={lr:.2e}  acc={acc:.4f}\")\n",
    "\n",
    "        best = float(np.max(accs))\n",
    "        best_lr = float(LR_GRID[int(np.argmax(accs))])\n",
    "\n",
    "        results_rank[kind][\"ranks\"][str(r)] = {\n",
    "            \"lrs\": [float(x) for x in LR_GRID],\n",
    "            \"acc\": accs,\n",
    "            \"best\": best,\n",
    "            \"best_lr\": best_lr,\n",
    "        }\n",
    "\n",
    "        print(f\"BEST for r={r}: acc={best:.4f} at lr={best_lr:.2e}\")\n",
    "\n",
    "# save json\n",
    "with open(\"artifacts/rank_x_lr_results.json\", \"w\") as f:\n",
    "    json.dump(results_rank, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved: artifacts/rank_x_lr_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots:\n",
      " - artifacts/heatmap_lora.png\n",
      " - artifacts/heatmap_dora.png\n",
      " - artifacts/heatmap_pissa.png\n",
      " - artifacts/heatmap_milora.png\n",
      " - artifacts/best_of_lr_vs_rank.png\n"
     ]
    }
   ],
   "source": [
    "# --- Plot rank×LR heatmaps + best-of-LR vs rank ---\n",
    "\n",
    "import json, os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_dir = \"artifacts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load results if needed\n",
    "if \"results_rank\" not in globals():\n",
    "    with open(\"artifacts/rank_x_lr_results.json\", \"r\") as f:\n",
    "        results_rank = json.load(f)\n",
    "\n",
    "# Common axes\n",
    "RANKS = sorted([int(r) for r in next(iter(results_rank.values()))[\"ranks\"].keys()])\n",
    "LR_GRID = np.array(next(iter(next(iter(results_rank.values()))[\"ranks\"].values()))[\"lrs\"], dtype=float)\n",
    "logLR = np.log10(LR_GRID)\n",
    "\n",
    "def make_heatmap(kind):\n",
    "    name = results_rank[kind][\"name\"]\n",
    "    # Matrix: rows=ranks, cols=lrs\n",
    "    M = []\n",
    "    for r in RANKS:\n",
    "        acc = results_rank[kind][\"ranks\"][str(r)][\"acc\"]\n",
    "        M.append(acc)\n",
    "    M = np.array(M, dtype=float)\n",
    "\n",
    "    plt.figure(figsize=(9, 4.8))\n",
    "    im = plt.imshow(\n",
    "        M,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        interpolation=\"nearest\",\n",
    "        extent=[logLR.min(), logLR.max(), min(RANKS), max(RANKS)],\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Validation accuracy\")\n",
    "    plt.xlabel(\"log10(LR)\")\n",
    "    plt.ylabel(\"Rank r\")\n",
    "    plt.title(f\"Rank × LR heatmap — {name}\")\n",
    "    plt.tight_layout()\n",
    "    path = os.path.join(out_dir, f\"heatmap_{kind}.png\")\n",
    "    plt.savefig(path, dpi=220)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "def make_best_curve():\n",
    "    plt.figure(figsize=(9, 4.8))\n",
    "    for kind in results_rank.keys():\n",
    "        name = results_rank[kind][\"name\"]\n",
    "        bests = [results_rank[kind][\"ranks\"][str(r)][\"best\"] for r in RANKS]\n",
    "        plt.plot(RANKS, bests, marker=\"o\", label=name)\n",
    "\n",
    "    plt.xlabel(\"Rank r\")\n",
    "    plt.ylabel(\"Best val accuracy (over LR sweep)\")\n",
    "    plt.title(\"Best-of-LR vs rank\")\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    path = os.path.join(out_dir, \"best_of_lr_vs_rank.png\")\n",
    "    plt.savefig(path, dpi=220)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "# Make heatmaps\n",
    "paths = []\n",
    "for kind in results_rank.keys():\n",
    "    paths.append(make_heatmap(kind))\n",
    "\n",
    "# Make best-vs-rank\n",
    "paths.append(make_best_curve())\n",
    "\n",
    "print(\"Saved plots:\")\n",
    "for p in paths:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d0cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normalized heatmaps:\n",
      " - artifacts/heatmap_normalized_lora.png\n",
      " - artifacts/heatmap_normalized_dora.png\n",
      " - artifacts/heatmap_normalized_pissa.png\n",
      " - artifacts/heatmap_normalized_milora.png\n"
     ]
    }
   ],
   "source": [
    "# --- Normalized Rank × LR heatmaps (relative to per-rank best) ---\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_dir = \"artifacts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load results if not already in memory\n",
    "if \"results_rank\" not in globals():\n",
    "    with open(\"artifacts/rank_x_lr_results.json\", \"r\") as f:\n",
    "        results_rank = json.load(f)\n",
    "\n",
    "# Common axes\n",
    "RANKS = sorted([int(r) for r in next(iter(results_rank.values()))[\"ranks\"].keys()])\n",
    "LR_GRID = np.array(\n",
    "    next(iter(next(iter(results_rank.values()))[\"ranks\"].values()))[\"lrs\"],\n",
    "    dtype=float\n",
    ")\n",
    "logLR = np.log10(LR_GRID)\n",
    "\n",
    "def normalized_heatmap(kind):\n",
    "    name = results_rank[kind][\"name\"]\n",
    "\n",
    "    # Build matrix: rows = ranks, cols = LRs\n",
    "    M = []\n",
    "    for r in RANKS:\n",
    "        acc = np.array(results_rank[kind][\"ranks\"][str(r)][\"acc\"], dtype=float)\n",
    "        acc_norm = acc / (acc.max() + 1e-12)   # normalize per rank\n",
    "        M.append(acc_norm)\n",
    "    M = np.array(M)\n",
    "\n",
    "    plt.figure(figsize=(9, 4.8))\n",
    "    im = plt.imshow(\n",
    "        M,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        interpolation=\"nearest\",\n",
    "        extent=[logLR.min(), logLR.max(), min(RANKS), max(RANKS)],\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label(\"Relative performance (per-rank max = 1.0)\")\n",
    "\n",
    "    plt.xlabel(\"log10(LR)\")\n",
    "    plt.ylabel(\"Rank r\")\n",
    "    plt.title(f\"Normalized Rank × LR sensitivity — {name}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(out_dir, f\"heatmap_normalized_{kind}.png\")\n",
    "    plt.savefig(path, dpi=220)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "paths = []\n",
    "for kind in results_rank.keys():\n",
    "    paths.append(normalized_heatmap(kind))\n",
    "\n",
    "print(\"Saved normalized heatmaps:\")\n",
    "for p in paths:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebbda6",
   "metadata": {},
   "source": [
    "### Summary & Next Steps\n",
    "\n",
    "**Observations**\n",
    "\n",
    "Our experiment successfully replicated the core findings of the paper \"Learning Rate Matters.\" Looking at the \"Performance vs. Learning Rate\" plot, we can draw two key conclusions:\n",
    "\n",
    "1.  **Optimal Learning Rates Differ:** The performance curves for each method peak at different points. For instance, PiSSA and MiLoRA, which modify the base weights, tended to prefer a lower optimal learning rate compared to vanilla LoRA in our experiment. This confirms that a single, fixed learning rate is not fair for comparing these methods.\n",
    "\n",
    "2.  **Peak Performance is Similar:** Crucially, once each method is tuned to its optimal learning rate, their peak accuracies are very close. In our run, all methods achieved a peak accuracy within a narrow range, demonstrating that the more complex initialization or architectural changes did not provide a significant advantage over a well-tuned vanilla LoRA.\n",
    "\n",
    "The training loss curves also show that at their respective optimal learning rates, all methods converge to a similar low loss value, with no single method showing a dramatically faster or more stable convergence profile than the others.\n",
    "\n",
    "**Limitations and Full-Scale Differences**\n",
    "\n",
    "This notebook used a small model and a synthetic dataset for educational purposes. At the full scale of the paper (e.g., a 7B parameter Llama model on a complex task), a few things would change:\n",
    "*   **Computational Cost:** The hyperparameter sweep would be immensely more expensive, which is precisely the problem the paper highlights—many researchers avoid it due to cost, leading to unfair comparisons.\n",
    "*   **More Pronounced Effects:** The subtle differences in optimal learning rates and the sharpness of the performance peaks would likely become even more pronounced with larger models and more complex data.\n",
    "*   **Other Hyperparameters:** At scale, other factors like LoRA rank `r`, alpha `α`, and optimizer settings (like weight decay) would also interact with the learning rate, making the tuning process even more critical.\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "This implementation provides a solid foundation for further exploration. Here are some concrete ideas for extending it:\n",
    "\n",
    "1.  **Vary LoRA Rank:** The paper also notes rank-dependent behaviors. One could extend the experiment loop to sweep over different values of `r` (e.g., 4, 8, 16, 32) in addition to the learning rate to create a 2D heatmap of performance.\n",
    "2.  **Implement Hessian Analysis:** Section 5 of the paper provides a theoretical justification for why different methods need different learning rates by analyzing the Hessian matrix of the loss function. One could implement a simplified version of this using PyTorch's autograd capabilities to estimate the maximum eigenvalue (sharpness) at initialization for each method and see if it inversely correlates with the optimal learning rate we found experimentally.\n",
    "3.  **Explore Other Tasks:** The same experimental setup could be applied to a different task, such as text summarization or classification, to see if the conclusions hold across different domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
